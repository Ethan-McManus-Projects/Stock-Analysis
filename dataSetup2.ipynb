{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stock data: 100%|██████████| 9/9 [00:11<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete.\n",
      "Saving combined data to compressed binary file...\n",
      "Combined data saved to 'combined_data_test.pkl.gz'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "gdelt_data = pd.read_csv('gdelt_data_test/gdelt_data_test.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data_test/stock_data_test.csv')\n",
    "\n",
    "# Convert date columns to datetime format and sort\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "gdelt_data = gdelt_data.sort_values(by='SQLDATE')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Load lookup tables\n",
    "with open('cameo_embeddings.json', 'r') as f:\n",
    "    event_lookup = json.load(f)\n",
    "with open('actor_embeddings_cleaned.json', 'r') as f:\n",
    "    actor_lookup = json.load(f)\n",
    "\n",
    "# Function to convert all values to int, float, or string (for dates)\n",
    "def convert_values(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_values(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_values(item) for item in data]\n",
    "    elif isinstance(data, (int, float)):\n",
    "        return data\n",
    "    elif isinstance(data, pd.Timestamp):\n",
    "        return data.strftime('%Y-%m-%d')\n",
    "    elif isinstance(data, str):\n",
    "        try:\n",
    "            # Try to convert to int\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # Try to convert to float\n",
    "                return float(data)\n",
    "            except ValueError:\n",
    "                # If conversion fails, return the original string\n",
    "                return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Initialize lists to store combined data and labels\n",
    "combined_data = []\n",
    "labels = []\n",
    "\n",
    "# Process the data for all date entries\n",
    "last_processed_date = None\n",
    "for i in tqdm(range(1, len(stock_data)), desc=\"Processing stock data\"):\n",
    "    try:\n",
    "        current_date = stock_data.iloc[i]['Date']\n",
    "        previous_close = stock_data.iloc[i - 1]['Close']\n",
    "        current_close = stock_data.iloc[i]['Close']\n",
    "        label = 1 if current_close > previous_close else 0\n",
    "\n",
    "        # Filter relevant entries from gdelt_data for the current date\n",
    "        relevant_entries = gdelt_data[gdelt_data['SQLDATE'] == current_date].copy()\n",
    "\n",
    "        # Process relevant entries\n",
    "        for col in ['Actor1Name', 'Actor2Name', 'EventCode']:\n",
    "            if col in ['Actor1Name', 'Actor2Name']:\n",
    "                # Perform the mapping using apply and lambda\n",
    "                relevant_entries[col] = relevant_entries[col].apply(\n",
    "                    lambda x: [actor_lookup.get(item, item) for item in x] if isinstance(x, list) else actor_lookup.get(x, x)\n",
    "                )\n",
    "\n",
    "                # Check for missing values\n",
    "                if relevant_entries[col].isnull().any():\n",
    "                    raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "            elif col == 'EventCode':\n",
    "                # Perform the mapping using apply and lambda\n",
    "                relevant_entries[col] = relevant_entries[col].astype(str).apply(\n",
    "                    lambda x: [event_lookup.get(item, item) for item in x] if isinstance(x, list) else event_lookup.get(x, x)\n",
    "                )\n",
    "\n",
    "                # Check for missing values\n",
    "                if relevant_entries[col].isnull().any():\n",
    "                    raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "\n",
    "        # Convert relevant entries to a list of dictionaries for JSON serialization\n",
    "        relevant_entries_dict = relevant_entries.to_dict(orient='records')\n",
    "\n",
    "        # Convert all values to int, float, or string (for dates)\n",
    "        relevant_entries_dict = convert_values(relevant_entries_dict)\n",
    "\n",
    "        # Append to the lists, converting the date to a string\n",
    "        combined_data.append({\n",
    "            'date': convert_values(current_date),\n",
    "            'label': label,\n",
    "            'data': relevant_entries_dict\n",
    "        })\n",
    "        labels.append(label)\n",
    "\n",
    "        # Update the last processed date\n",
    "        last_processed_date = current_date\n",
    "\n",
    "    except ValueError as e:\n",
    "        # Handle specific ValueError exceptions without printing\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # Print other exceptions\n",
    "        print(f\"Error processing index {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "\n",
    "# Save to compressed binary file using joblib with gzip\n",
    "print(\"Saving combined data to compressed binary file...\")\n",
    "joblib.dump(combined_data, 'combined_data_test.pkl.gz', compress=('gzip', 3))\n",
    "print(\"Combined data saved to 'combined_data_test.pkl.gz'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving entries to JSON: 100%|██████████| 9/9 [01:19<00:00,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries saved to 'first_entries_2015.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the compressed binary file\n",
    "combined_data_2015 = joblib.load('combined_data_test.pkl.gz')\n",
    "\n",
    "# Extract the first 5 entries\n",
    "first_5_entries = combined_data_2015[:]\n",
    "\n",
    "# Save the first 5 entries to a JSON file with a progress bar\n",
    "with open('first_entries_2015.json', 'w') as json_file:\n",
    "    json_file.write('[\\n')  # Start the JSON array\n",
    "    for i, entry in enumerate(tqdm(first_5_entries, desc=\"Saving entries to JSON\")):\n",
    "        json.dump(entry, json_file, indent=4)\n",
    "        if i < len(first_5_entries) - 1:\n",
    "            json_file.write(',\\n')  # Add a comma after each entry except the last one\n",
    "        else:\n",
    "            json_file.write('\\n')  # No comma after the last entry\n",
    "    json_file.write(']\\n')  # End the JSON array\n",
    "\n",
    "print(\"First 5 entries saved to 'first_entries_2015.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in Actor1Name: 300\n",
      "Number of elements in Actor2Name: 300\n",
      "Number of elements in EventCode: 512\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('first_entries_2015.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Get the first entry\n",
    "first_entry = data[0]\n",
    "\n",
    "# Extract the first element of the 'data' list\n",
    "first_data_entry = first_entry['data'][0]\n",
    "\n",
    "# Extract the relevant columns\n",
    "actor1_name = first_data_entry['Actor1Name']\n",
    "actor2_name = first_data_entry['Actor2Name']\n",
    "event_code = first_data_entry['EventCode']\n",
    "\n",
    "# Print the number of elements in each column\n",
    "print(f\"Number of elements in Actor1Name: {len(actor1_name)}\")\n",
    "print(f\"Number of elements in Actor2Name: {len(actor2_name)}\")\n",
    "print(f\"Number of elements in EventCode: {len(event_code)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "important data splitting cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gdelt_data_by_year/gdelt_data_2015.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2016.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2017.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2018.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2019.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2020.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2021.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2022.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2023.csv\n",
      "Saved gdelt_data_by_year/gdelt_data_2024.csv\n",
      "Saved stock_data_by_year/stock_data_2015.csv\n",
      "Saved stock_data_by_year/stock_data_2016.csv\n",
      "Saved stock_data_by_year/stock_data_2017.csv\n",
      "Saved stock_data_by_year/stock_data_2018.csv\n",
      "Saved stock_data_by_year/stock_data_2019.csv\n",
      "Saved stock_data_by_year/stock_data_2020.csv\n",
      "Saved stock_data_by_year/stock_data_2021.csv\n",
      "Saved stock_data_by_year/stock_data_2022.csv\n",
      "Saved stock_data_by_year/stock_data_2023.csv\n",
      "Saved stock_data_by_year/stock_data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "gdelt_data = pd.read_csv('gdelt_data_cleaned.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Create directories to save the yearly data if they don't exist\n",
    "os.makedirs('gdelt_data_by_year', exist_ok=True)\n",
    "os.makedirs('stock_data_by_year', exist_ok=True)\n",
    "\n",
    "# Split gdelt_data by year and save to separate files\n",
    "for year, data in gdelt_data.groupby(gdelt_data['SQLDATE'].dt.year):\n",
    "    filename = f'gdelt_data_by_year/gdelt_data_{year}.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Split stock_data by year and save to separate files\n",
    "for year, data in stock_data.groupby(stock_data['Date'].dt.year):\n",
    "    filename = f'stock_data_by_year/stock_data_{year}.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stock_data_test/stock_data_test.csv\n",
      "Saved gdelt_data_test/gdelt_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "gdelt_data = pd.read_csv('gdelt_data_cleaned.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Extract the first 10 dates from the stock data\n",
    "first_10_dates = stock_data['Date'].sort_values().unique()[:10]\n",
    "\n",
    "# Filter stock data to include only the first 10 dates\n",
    "stock_data_test = stock_data[stock_data['Date'].isin(first_10_dates)]\n",
    "\n",
    "# Filter GDELT data to include only entries corresponding to the first 10 dates\n",
    "gdelt_data_test = gdelt_data[gdelt_data['SQLDATE'].isin(first_10_dates)]\n",
    "\n",
    "# Create directories to save the test data if they don't exist\n",
    "os.makedirs('gdelt_data_test', exist_ok=True)\n",
    "os.makedirs('stock_data_test', exist_ok=True)\n",
    "\n",
    "# Save the filtered stock data to a test file\n",
    "stock_data_test_filename = 'stock_data_test/stock_data_test.csv'\n",
    "stock_data_test.to_csv(stock_data_test_filename, index=False)\n",
    "print(f\"Saved {stock_data_test_filename}\")\n",
    "\n",
    "# Save the filtered GDELT data to a test file\n",
    "gdelt_data_test_filename = 'gdelt_data_test/gdelt_data_test.csv'\n",
    "gdelt_data_test.to_csv(gdelt_data_test_filename, index=False)\n",
    "print(f\"Saved {gdelt_data_test_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StockTradeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
