{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# formatting and load data\n",
    "gdelt_data = pd.read_csv('gdelt_data_test/gdelt_data_test.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data_test/stock_data_test.csv')\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "gdelt_data = gdelt_data.sort_values(by='SQLDATE')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# load hashmaps\n",
    "with open('cameo_embeddings.json', 'r') as f:\n",
    "    event_lookup = json.load(f)\n",
    "with open('actor_embeddings_cleaned.json', 'r') as f:\n",
    "    actor_lookup = json.load(f)\n",
    "\n",
    "# convert values to required form for json readability and for better processing\n",
    "def convert_values(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_values(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_values(item) for item in data]\n",
    "    elif isinstance(data, (int, float)):\n",
    "        return data\n",
    "    elif isinstance(data, pd.Timestamp):\n",
    "        return data.strftime('%Y-%m-%d')\n",
    "    elif isinstance(data, str):\n",
    "        try:\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(data)\n",
    "            except ValueError:\n",
    "                # default\n",
    "                return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# initialize lists\n",
    "combined_data = []\n",
    "labels = []\n",
    "\n",
    "# process all data\n",
    "last_processed_date = None\n",
    "for i in tqdm(range(1, len(stock_data)), desc=\"Processing stock data\"):\n",
    "    try:\n",
    "        current_date = stock_data.iloc[i]['Date']\n",
    "        previous_close = stock_data.iloc[i - 1]['Close']\n",
    "        current_close = stock_data.iloc[i]['Close']\n",
    "        label = 1 if current_close > previous_close else 0\n",
    "\n",
    "        # get entries for date\n",
    "        relevant_entries = gdelt_data[gdelt_data['SQLDATE'] == current_date].copy()\n",
    "\n",
    "        for col in ['Actor1Name', 'Actor2Name', 'EventCode']:\n",
    "            if col in ['Actor1Name', 'Actor2Name']:\n",
    "                # lambda function \n",
    "                relevant_entries[col] = relevant_entries[col].apply(\n",
    "                    lambda x: [actor_lookup.get(item, item) for item in x] if isinstance(x, list) else actor_lookup.get(x, x)\n",
    "                )\n",
    "\n",
    "                # Check for missing or nan\n",
    "                if relevant_entries[col].isnull().any():\n",
    "                    raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "            elif col == 'EventCode':\n",
    "                # use lambda\n",
    "                relevant_entries[col] = relevant_entries[col].astype(str).apply(\n",
    "                    lambda x: [event_lookup.get(item, item) for item in x] if isinstance(x, list) else event_lookup.get(x, x)\n",
    "                )\n",
    "                # Check again for eventcode\n",
    "                if relevant_entries[col].isnull().any():\n",
    "                    raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "\n",
    "        # Convert relevant entries to a list of dictionaries for JSON serialization and fast processing\n",
    "        relevant_entries_dict = relevant_entries.to_dict(orient='records')\n",
    "        # convert values call\n",
    "        relevant_entries_dict = convert_values(relevant_entries_dict)\n",
    "\n",
    "        # Append to the lists, converting the date to a string using convert function\n",
    "        combined_data.append({\n",
    "            'date': convert_values(current_date),\n",
    "            'label': label,\n",
    "            'data': relevant_entries_dict\n",
    "        })\n",
    "        labels.append(label)\n",
    "\n",
    "        # Update the last processed date\n",
    "        last_processed_date = current_date\n",
    "\n",
    "    except ValueError as e:\n",
    "        # general error catch\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        # Print other exceptions, print statments taken from other places for proper formatting\n",
    "        print(f\"Error processing index {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "\n",
    "# save out\n",
    "print(\"Saving combined data to compressed binary file...\")\n",
    "joblib.dump(combined_data, 'combined_data_test.pkl.gz', compress=('gzip', 3))\n",
    "print(\"Combined data saved to 'combined_data_test.pkl.gz'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "better processing cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load files\n",
    "gdelt_data = pd.read_csv('gdelt_data_test/gdelt_data_test.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data_test/stock_data_test.csv')\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "gdelt_data = gdelt_data.sort_values(by='SQLDATE')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# load hashmaps\n",
    "with open('cameo_embeddings.json', 'r') as f:\n",
    "    event_lookup = json.load(f)\n",
    "with open('actor_embeddings_cleaned.json', 'r') as f:\n",
    "    actor_lookup = json.load(f)\n",
    "\n",
    "# convert to correct format function\n",
    "def convert_values(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: convert_values(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_values(item) for item in data]\n",
    "    elif isinstance(data, (int, float)):\n",
    "        return data\n",
    "    elif isinstance(data, pd.Timestamp):\n",
    "        return data.strftime('%Y-%m-%d')\n",
    "    elif isinstance(data, str):\n",
    "        try:\n",
    "            return int(data)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return float(data)\n",
    "            except ValueError:\n",
    "                # default\n",
    "                return data\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Initialize lists \n",
    "combined_data = []\n",
    "labels = []\n",
    "\n",
    "# Create HDF5 file\n",
    "with h5py.File('combined_data_test.h5', 'w') as h5f:\n",
    "    # process all date data\n",
    "    last_processed_date = None\n",
    "    for i in tqdm(range(1, len(stock_data)), desc=\"Processing stock data\"):\n",
    "        try:\n",
    "            current_date = stock_data.iloc[i]['Date']\n",
    "            previous_close = stock_data.iloc[i - 1]['Close']\n",
    "            current_close = stock_data.iloc[i]['Close']\n",
    "            label = 1 if current_close > previous_close else 0\n",
    "\n",
    "            # relevnt entries filtered per date\n",
    "            relevant_entries = gdelt_data[gdelt_data['SQLDATE'] == current_date].copy()\n",
    "            for col in ['Actor1Name', 'Actor2Name', 'EventCode']:\n",
    "                if col in ['Actor1Name', 'Actor2Name']:\n",
    "                    # use lambda to map\n",
    "                    relevant_entries[col] = relevant_entries[col].apply(\n",
    "                        lambda x: [actor_lookup.get(item, item) for item in x] if isinstance(x, list) else actor_lookup.get(x, x)\n",
    "                    )\n",
    "\n",
    "                    # Check for missing\n",
    "                    if relevant_entries[col].isnull().any():\n",
    "                        raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "                    \n",
    "                elif col == 'EventCode':\n",
    "                    relevant_entries[col] = relevant_entries[col].astype(str).apply(\n",
    "                        lambda x: [event_lookup.get(item, item) for item in x] if isinstance(x, list) else event_lookup.get(x, x)\n",
    "                    )\n",
    "\n",
    "                    if relevant_entries[col].isnull().any():\n",
    "                        raise ValueError(f\"Missing data found in column {col} for date {current_date}\")\n",
    "\n",
    "            # Convert relevant entries to a list of dictionaries for JSON serialization and fast processing\n",
    "            relevant_entries_dict = relevant_entries.to_dict(orient='records')\n",
    "\n",
    "            # Convert values call\n",
    "            relevant_entries_dict = convert_values(relevant_entries_dict)\n",
    "            # create structured datsaet\n",
    "            date_group = h5f.create_group(str(current_date.date()))\n",
    "            date_group.attrs['label'] = label\n",
    "            for j, entry in enumerate(relevant_entries_dict):\n",
    "                entry_group = date_group.create_group(f'entry_{j}')\n",
    "                for key, value in entry.items():\n",
    "                    entry_group.create_dataset(key, data=value)\n",
    "\n",
    "            # Update the last processed date\n",
    "            last_processed_date = current_date\n",
    "\n",
    "        except ValueError as e:\n",
    "            # handle general errors\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # print errors, proper formatting\n",
    "            print(f\"Error processing index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully testing cell, used for test cases and debugging in memory exlosion problem in the stock analysis file\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (int, float, str, bool)):\n",
    "        return obj\n",
    "    elif isinstance(obj, (pd.Timestamp, pd.Timedelta)):\n",
    "        return str(obj)\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "with h5py.File('combined_data_group_1.h5', 'r') as h5f:\n",
    "    first_three_entries = []\n",
    "    date_groups = list(h5f.keys())\n",
    "    \n",
    "    for date_group_name in date_groups[:3]:\n",
    "        date_group = h5f[date_group_name]\n",
    "        entry_dict = {\n",
    "            'date': date_group_name,\n",
    "            'label': date_group.attrs['label'],\n",
    "            'data': []\n",
    "        }\n",
    "        for entry_name in date_group.keys():\n",
    "            entry_group = date_group[entry_name]\n",
    "            entry_data = {}\n",
    "            for key in entry_group.keys():\n",
    "                entry_data[key] = convert_to_serializable(entry_group[key][()])\n",
    "            entry_dict['data'].append(entry_data)\n",
    "        \n",
    "        first_three_entries.append(entry_dict)\n",
    "\n",
    "json_data = json.dumps(convert_to_serializable(first_three_entries), indent=4)\n",
    "with open('first_three_entries.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print structure of the HDF5 file for testing and debugging purposes\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with open('first_entries_2015.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "with h5py.File('data.h5', 'w') as h5f:\n",
    "    for entry in json_data:\n",
    "        date_group = h5f.create_group(entry['date'])\n",
    "        date_group.attrs['label'] = entry['label']\n",
    "        \n",
    "        data_group = date_group.create_group('data')\n",
    "        for i, data_entry in enumerate(entry['data']):\n",
    "            entry_group = data_group.create_group(f'entry_{i}')\n",
    "            for key, value in data_entry.items():\n",
    "                if isinstance(value, list):\n",
    "                    entry_group.create_dataset(key, data=np.array(value))\n",
    "                else:\n",
    "                    entry_group.attrs[key] = value\n",
    "                    \n",
    "with h5py.File('data.h5', 'r') as h5f:\n",
    "    def print_structure(name, obj):\n",
    "        print(name)\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(f\"  {key}: {val}\")\n",
    "    h5f.visititems(print_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumps the HDF5 file to a JSON file so that I can read it and assess if the processing was done correctly (or correctly enough to be passable)\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def h5_to_dict(h5file):\n",
    "    def recursively_convert_group(group):\n",
    "        result = {}\n",
    "        for key, item in group.items():\n",
    "            if isinstance(item, h5py.Group):\n",
    "                result[key] = recursively_convert_group(item)\n",
    "\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                result[key] = item[()].tolist()\n",
    "\n",
    "        for key, val in group.attrs.items():\n",
    "            result[key] = val.tolist() if isinstance(val, np.ndarray) else val\n",
    "\n",
    "        return result\n",
    "    \n",
    "    data_dict = {}\n",
    "\n",
    "    for key, item in h5file.items():\n",
    "        data_dict[key] = recursively_convert_group(item)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "\n",
    "    if isinstance(obj, (np.integer, np.floating)):\n",
    "        return obj.item()\n",
    "    \n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    \n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(i) for i in obj]\n",
    "    \n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    \n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "with h5py.File('data.h5', 'r') as h5f:\n",
    "    data_dict = h5_to_dict(h5f)\n",
    "\n",
    "json_data = []\n",
    "for date, date_group in data_dict.items():\n",
    "    entry = {\n",
    "        'date': date,\n",
    "        'label': convert_to_serializable(date_group.pop('label')),\n",
    "        'data': []\n",
    "    }\n",
    "\n",
    "    for entry_key, entry_group in date_group['data'].items():\n",
    "        entry['data'].append(convert_to_serializable(entry_group))\n",
    "\n",
    "    json_data.append(entry)\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds labels for date entries to help me check if the data was processed correctly after entry 1\n",
    "\n",
    "import json\n",
    "\n",
    "with open('first_three_entries.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "def find_labels(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        \n",
    "        lines = f.readlines()\n",
    "    \n",
    "    label_lines = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if '\"label\":' in line:\n",
    "            label_lines.append(i + 1)\n",
    "    return label_lines\n",
    "\n",
    "label_lines = find_labels('first_three_entries.json')\n",
    "print(f\"Labels found at lines: {label_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the json file made above, make a smaller json file that only has the first three entries because the files are too large to be easily viewed\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load compressed file\n",
    "combined_data_2015 = joblib.load('combined_data_test.pkl.gz')\n",
    "\n",
    "# Extract the first 5 entries\n",
    "first_5_entries = combined_data_2015[:100]\n",
    "\n",
    "# add stuff for json formatting\n",
    "with open('first_entries_2015.json', 'w') as json_file:\n",
    "    json_file.write('[\\n') \n",
    "    for i, entry in enumerate(tqdm(first_5_entries, desc=\"Saving entries to JSON\")):\n",
    "        json.dump(entry, json_file, indent=4)\n",
    "        if i < len(first_5_entries) - 1:\n",
    "            json_file.write(',\\n')\n",
    "        else:\n",
    "            json_file.write('\\n')\n",
    "    json_file.write(']\\n')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archival testing, or could be used for json testing, prints number of items in each element of the first entry, important for finding out if the data was processed correctly and finding the size of the arrays since I forgot\n",
    "\n",
    "import json\n",
    "\n",
    "with open('first_entries_2015.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "first_entry = data[0]\n",
    "\n",
    "# get first entry in data, no longer used like this\n",
    "first_data_entry = first_entry['data'][0]\n",
    "\n",
    "actor1_name = first_data_entry['Actor1Name']\n",
    "actor2_name = first_data_entry['Actor2Name']\n",
    "event_code = first_data_entry['EventCode']\n",
    "print(f\"Number of elements in Actor1Name: {len(actor1_name)}\")\n",
    "print(f\"Number of elements in Actor2Name: {len(actor2_name)}\")\n",
    "print(f\"Number of elements in EventCode: {len(event_code)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "important data splitting cell"
    ]
   },
   "outputs": [],
   "source": [
    "# chunks the data as wanted, important cell\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "gdelt_data = pd.read_csv('gdelt_data_cleaned.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data.csv')\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "os.makedirs('gdelt_data_by_chunks', exist_ok=True)\n",
    "os.makedirs('stock_data_by_chunks', exist_ok=True)\n",
    "\n",
    "# Split stock_data into chunks of 100 entries and save to separate files, uses date range to find which entres to append for each date, depends on previous data, two different files\n",
    "chunk_size = 100\n",
    "for i, chunk in enumerate(range(0, stock_data.shape[0], chunk_size)):\n",
    "    chunk_data = stock_data.iloc[chunk:chunk + chunk_size]\n",
    "    filename = f'stock_data_by_chunks/stock_data_chunk_{i + 1}.csv'\n",
    "    chunk_data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "    start_date = chunk_data['Date'].min()\n",
    "    end_date = chunk_data['Date'].max()\n",
    "    gdelt_chunk = gdelt_data[(gdelt_data['SQLDATE'] >= start_date) & (gdelt_data['SQLDATE'] <= end_date)]\n",
    "    gdelt_filename = f'gdelt_data_by_chunks/gdelt_data_chunk_{i + 1}.csv'\n",
    "    gdelt_chunk.to_csv(gdelt_filename, index=False)\n",
    "    print(f\"Saved {gdelt_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does essentially the same thing as the file above, but it was used for testing earlier, and it only makes one smaller chunk\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "gdelt_data = pd.read_csv('gdelt_data_cleaned.csv', low_memory=False)\n",
    "stock_data = pd.read_csv('stock_data.csv')\n",
    "gdelt_data['SQLDATE'] = pd.to_datetime(gdelt_data['SQLDATE'], format='%Y-%m-%d')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], format='%Y-%m-%d')\n",
    "\n",
    "first_10_dates = stock_data['Date'].sort_values().unique()[:10]\n",
    "stock_data_test = stock_data[stock_data['Date'].isin(first_10_dates)]\n",
    "gdelt_data_test = gdelt_data[gdelt_data['SQLDATE'].isin(first_10_dates)]\n",
    "\n",
    "os.makedirs('gdelt_data_test', exist_ok=True)\n",
    "os.makedirs('stock_data_test', exist_ok=True)\n",
    "\n",
    "stock_data_test_filename = 'stock_data_test/stock_data_test.csv'\n",
    "stock_data_test.to_csv(stock_data_test_filename, index=False)\n",
    "print(f\"Saved {stock_data_test_filename}\")\n",
    "\n",
    "gdelt_data_test_filename = 'gdelt_data_test/gdelt_data_test.csv'\n",
    "gdelt_data_test.to_csv(gdelt_data_test_filename, index=False)\n",
    "print(f\"Saved {gdelt_data_test_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "testing faliure"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell was intended to test how large the pkl file was unzipped, but it failed, and I did not end up using pkl whatsoever, so it is mostly useless\n",
    "\n",
    "import joblib\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "data = joblib.load('combined_data_test.pkl.gz')\n",
    "\n",
    "def get_memory_size(obj):\n",
    "    return sys.getsizeof(obj)\n",
    "\n",
    "def get_pandas_memory_usage(df):\n",
    "    return df.memory_usage(deep=True).sum()\n",
    "\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    memory_size = get_pandas_memory_usage(data)\n",
    "else:\n",
    "    memory_size = get_memory_size(data)\n",
    "\n",
    "print(f\"Estimated memory size: {memory_size / (1000 ** 2):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StockTradeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
